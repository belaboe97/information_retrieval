{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for evaluating Information Retrieval Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all required stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaltools import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the code cell below to study the different metrics for IR systems evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is revisiting the concepts discussed at class about IR systems evaluation in a practical manner. The code embedded in this notebook automatically generates a set of test queries (of size 'number_of_queries') and issues each of this query to a random IR retrieval system. For each launched query, the IR system generates a ranking of all documents in the collection (containing 'collection_size' documents), together with the relevance judgements for each document in the ranking (i.e. experts judgements indicating whether that document is truly relevant for the query or not)\n",
    "\n",
    "Let us suppose that we want to evaluate the target IR with a set of 9 queries over a collection composed of 500 documents. In such scenario, we would set the variables 'number_of_queries' and 'collection_size' to the values 9 and 500 respectively.\n",
    "\n",
    "You can leave both variables set at 0 to have random sizes for both the size of the query set and the collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_queries=9\n",
    "collection_size=500\n",
    "Q,R=gen_eval_dataset(number_of_queries, collection_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select a evaluation method set the variable 'method' of the cell below to any of the following values:\n",
    "\n",
    "* 'prec_rec': draw the Precision-Recall curve for each query;\n",
    "* 'r-prec': Determine the R-precision for each query;\n",
    "* 'map': Calculate the Mean Average Precision;\n",
    "* 'roc': Draw the Receiver-Operating-Characteristic for each query;\n",
    "* 'auc': Compute the Area Under the ROC curve;\n",
    "* 'all': all of the above;\n",
    "* 'clear': cleans the solution space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "method='prec_rec' \n",
    "evaluate(method,Q,R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
